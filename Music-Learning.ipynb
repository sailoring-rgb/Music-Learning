{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'midi2audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb Cell 2\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mIPython\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, Audio\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmidi2audio\u001b[39;00m \u001b[39mimport\u001b[39;00m FluidSynth\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmusic21\u001b[39;00m \u001b[39mimport\u001b[39;00m corpus, converter, instrument, note, stream, chord, duration\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'midi2audio'"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "from IPython.display import Image, Audio\n",
    "from midi2audio import FluidSynth\n",
    "from music21 import corpus, converter, instrument, note, stream, chord, duration\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape\n",
    "from keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\n",
    "from keras.layers import Multiply, Lambda, Softmax\n",
    "import keras.backend as K \n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbach_846\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.mid\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(dataset_name, filename)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/annaphens/Desktop/college/DeeL/Music-Learning/Music-Learning.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m original_score \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mparse(file)\u001b[39m.\u001b[39mchordify()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'converter' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name = '/input/classical-music/bach'\n",
    "filename = 'bach_846'\n",
    "file = \"{}/{}.mid\".format(dataset_name, filename)\n",
    "\n",
    "original_score = converter.parse(file).chordify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FluidSynth()\n",
    "file = '/input/classical-music/bach/bach_846.mid'\n",
    "fs.midi_to_audio(file, 'bach_846.wav')\n",
    "IPython.display.Audio(\"bach_846.wav\", 44100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.midi_to_audio('/input/classical-music/bach/bach_847.mid', 'bach_847.wav')\n",
    "IPython.display.Audio(\"bach_847.wav\", 44100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.midi_to_audio('/input/classical-music/bach/bach_850.mid', 'bach_850.wav')\n",
    "IPython.display.Audio(\"bach_850.wav\", 44100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_score.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "durations = []\n",
    "\n",
    "for element in original_score.flat:    \n",
    "    if isinstance(element, chord.Chord):\n",
    "        notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n",
    "        durations.append(element.duration.quarterLength)\n",
    "\n",
    "    if isinstance(element, note.Note):\n",
    "        if element.isRest:\n",
    "            notes.append(str(element.name))\n",
    "            durations.append(element.duration.quarterLength)\n",
    "        else:\n",
    "            notes.append(str(element.nameWithOctave))\n",
    "            durations.append(element.duration.quarterLength)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nduration', 'pitch')\n",
    "idx = 0\n",
    "for n,d in zip(notes,durations):\n",
    "    if idx < 50:\n",
    "        print(d, '\\t', n)\n",
    "    idx = idx + 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_list(data_folder):    \n",
    "    file_list = glob.glob(os.path.join(data_folder, \"*.mid\"))\n",
    "    parser = converter    \n",
    "    return file_list, parser  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(n_notes, n_durations, embed_size = 100, rnn_units = 256, use_attention = False):\n",
    "    notes_in = Input(shape = (None,))\n",
    "    durations_in = Input(shape = (None,))\n",
    "\n",
    "    x1 = Embedding(n_notes, embed_size)(notes_in)\n",
    "    x2 = Embedding(n_durations, embed_size)(durations_in) \n",
    "    x = Concatenate()([x1,x2])\n",
    "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "    if use_attention:\n",
    "        x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "        e = Dense(1, activation='tanh')(x)\n",
    "        e = Reshape([-1])(e)\n",
    "        alpha = Activation('softmax')(e)\n",
    "        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n",
    "        c = Multiply()([x, alpha_repeated])\n",
    "        c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)    \n",
    "    else:\n",
    "        c = LSTM(rnn_units)(x)\n",
    "                                    \n",
    "    notes_out = Dense(n_notes, activation = 'softmax', name = 'pitch')(c)\n",
    "    durations_out = Dense(n_durations, activation = 'softmax', name = 'duration')(c)\n",
    "   \n",
    "    model = Model([notes_in, durations_in], [notes_out, durations_out])    \n",
    "\n",
    "    if use_attention:\n",
    "        att_model = Model([notes_in, durations_in], alpha)\n",
    "    else:\n",
    "        att_model = None\n",
    "        \n",
    "    opti = RMSprop(lr = 0.001)\n",
    "    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'], optimizer=opti)\n",
    "\n",
    "    return model, att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct(elements):\n",
    "    # Get all pitch names\n",
    "    element_names = sorted(set(elements))\n",
    "    n_elements = len(element_names)\n",
    "    return (element_names, n_elements)\n",
    "\n",
    "def create_lookups(element_names):\n",
    "    # create dictionary to map notes and durations to integers\n",
    "    element_to_int = dict((element, number) for number, element in enumerate(element_names))\n",
    "    int_to_element = dict((number, element) for number, element in enumerate(element_names))\n",
    "    return (element_to_int, int_to_element)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, durations, lookups, distincts, seq_len =32):\n",
    "    note_to_int, int_to_note, duration_to_int, int_to_duration = lookups\n",
    "    note_names, n_notes, duration_names, n_durations = distincts\n",
    "\n",
    "    notes_network_input = []\n",
    "    notes_network_output = []\n",
    "    durations_network_input = []\n",
    "    durations_network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(len(notes) - seq_len):\n",
    "        notes_sequence_in = notes[i:i + seq_len]\n",
    "        notes_sequence_out = notes[i + seq_len]\n",
    "        notes_network_input.append([note_to_int[char] for char in notes_sequence_in])\n",
    "        notes_network_output.append(note_to_int[notes_sequence_out])\n",
    "\n",
    "        durations_sequence_in = durations[i:i + seq_len]\n",
    "        durations_sequence_out = durations[i + seq_len]\n",
    "        durations_network_input.append([duration_to_int[char] for char in durations_sequence_in])\n",
    "        durations_network_output.append(duration_to_int[durations_sequence_out])\n",
    "\n",
    "    n_patterns = len(notes_network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    notes_network_input = np.reshape(notes_network_input, (n_patterns, seq_len))\n",
    "    durations_network_input = np.reshape(durations_network_input, (n_patterns, seq_len))\n",
    "    network_input = [notes_network_input, durations_network_input]\n",
    "\n",
    "    notes_network_output = np_utils.to_categorical(notes_network_output, num_classes=n_notes)\n",
    "    durations_network_output = np_utils.to_categorical(durations_network_output, num_classes=n_durations)\n",
    "    network_output = [notes_network_output, durations_network_output]\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature):\n",
    "    if temperature == 0:\n",
    "        return np.argmax(preds)\n",
    "    else:\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        return np.random.choice(len(preds), p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# run params\n",
    "run_folder = '/kaggle/working'\n",
    "\n",
    "store_folder = os.path.join(run_folder, 'store')\n",
    "data_folder ='/input/classical-music/bach'\n",
    "\n",
    "if not os.path.exists('store'):\n",
    "    os.mkdir(os.path.join(run_folder, 'store'))\n",
    "    os.mkdir(os.path.join(run_folder, 'output'))\n",
    "    os.mkdir(os.path.join(run_folder, 'weights'))\n",
    "    os.mkdir(os.path.join(run_folder, 'viz'))\n",
    "\"\"\"\n",
    "data_folder ='/input/classical-music/bach'\n",
    "store_folder = '/store'\n",
    "\n",
    "mode = 'build'\n",
    "\n",
    "# data params\n",
    "intervals = range(1)\n",
    "seq_len = 32\n",
    "\n",
    "# model params\n",
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'build':    \n",
    "    music_list, parser = get_music_list(data_folder)\n",
    "    print(len(music_list), 'files in total')\n",
    "\n",
    "    notes = []\n",
    "    durations = []\n",
    "\n",
    "    for i, file in enumerate(music_list):\n",
    "        print(i+1, \"Parsing %s\" % file)\n",
    "        print(file)\n",
    "        original_score = parser.parse(file).chordify()        \n",
    "        for interval in intervals:\n",
    "            score = original_score.transpose(interval)\n",
    "\n",
    "            notes.extend(['START'] * seq_len)\n",
    "            durations.extend([0]* seq_len)\n",
    "\n",
    "            for element in score.flat:                \n",
    "                if isinstance(element, note.Note):\n",
    "                    if element.isRest:\n",
    "                        notes.append(str(element.name))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                    else:\n",
    "                        notes.append(str(element.nameWithOctave))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                        \n",
    "                if isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n",
    "                    durations.append(element.duration.quarterLength)\n",
    "\n",
    "    with open(os.path.join(store_folder, 'notes'), 'wb') as f:\n",
    "        pickle.dump(notes, f) \n",
    "    with open(os.path.join(store_folder, 'durations'), 'wb') as f:\n",
    "        pickle.dump(durations, f) \n",
    "else:\n",
    "    with open(os.path.join(store_folder, 'notes'), 'rb') as f:\n",
    "        notes = pickle.load(f)\n",
    "    with open(os.path.join(store_folder, 'durations'), 'rb') as f:\n",
    "        durations = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct sets of notes and durations\n",
    "note_names, n_notes = get_distinct(notes)\n",
    "duration_names, n_durations = get_distinct(durations)\n",
    "distincts = [note_names, n_notes, duration_names, n_durations]\n",
    "\n",
    "with open(os.path.join(store_folder, 'distincts'), 'wb') as f:\n",
    "    pickle.dump(distincts, f)\n",
    "\n",
    "# make the lookup dictionaries for notes and dictionaries and save\n",
    "note_to_int, int_to_note = create_lookups(note_names)\n",
    "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
    "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n",
    "\n",
    "with open(os.path.join(store_folder, 'lookups'), 'wb') as f:\n",
    "    pickle.dump(lookups, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nnote_to_int')\n",
    "for i, item in enumerate(note_to_int.items()):\n",
    "    if i < 10:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nduration_to_int')\n",
    "duration_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pitch input')\n",
    "print(network_input[0][0])\n",
    "print('duration input')\n",
    "print(network_input[1][0])\n",
    "print('pitch target')\n",
    "print(network_output[0][0])\n",
    "print('duration target')\n",
    "print(network_output[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, att_model = create_network(n_notes, n_durations, embed_size, rnn_units, use_attention)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=os.path.join('/output' ,'viz/model.png'), show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = os.path.join('/output', 'weights')\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    restore_best_weights=True,\n",
    "    patience = 10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1,\n",
    "    checkpoint2,\n",
    "    early_stopping\n",
    " ]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(network_input, network_output,\n",
    "          epochs=2000000, batch_size=32,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=callbacks_list,\n",
    "          shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction params\n",
    "notes_temp=0.5\n",
    "duration_temp = 0.5\n",
    "max_extra_notes = 50\n",
    "max_seq_len = 32\n",
    "seq_len = 32\n",
    "\n",
    "notes = ['START']\n",
    "durations = [0]\n",
    "\n",
    "if seq_len is not None:\n",
    "    notes = ['START'] * (seq_len - len(notes)) + notes\n",
    "    durations = [0] * (seq_len - len(durations)) + durations\n",
    "\n",
    "sequence_length = len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = []\n",
    "notes_input_sequence = []\n",
    "durations_input_sequence = []\n",
    "overall_preds = []\n",
    "\n",
    "for n, d in zip(notes,durations):\n",
    "    note_int = note_to_int[n]\n",
    "    duration_int = duration_to_int[d]\n",
    "    \n",
    "    notes_input_sequence.append(note_int)\n",
    "    durations_input_sequence.append(duration_int)\n",
    "    \n",
    "    prediction_output.append([n, d])\n",
    "    \n",
    "    if n != 'START':\n",
    "        midi_note = note.Note(n)\n",
    "        new_note = np.zeros(128)\n",
    "        new_note[midi_note.pitch.midi] = 1\n",
    "        overall_preds.append(new_note)\n",
    "\n",
    "att_matrix = np.zeros(shape = (max_extra_notes+sequence_length, max_extra_notes))\n",
    "\n",
    "for note_index in range(max_extra_notes):\n",
    "\n",
    "    prediction_input = [\n",
    "        np.array([notes_input_sequence])\n",
    "        , np.array([durations_input_sequence])\n",
    "       ]\n",
    "\n",
    "    notes_prediction, durations_prediction = model.predict(prediction_input, verbose=0)\n",
    "    if use_attention:\n",
    "        att_prediction = att_model.predict(prediction_input, verbose=0)[0]\n",
    "        att_matrix[(note_index-len(att_prediction)+sequence_length):(note_index+sequence_length), note_index] = att_prediction\n",
    "    \n",
    "    new_note = np.zeros(128)\n",
    "    \n",
    "    for idx, n_i in enumerate(notes_prediction[0]):\n",
    "        try:\n",
    "            note_name = int_to_note[idx]\n",
    "            midi_note = note.Note(note_name)\n",
    "            new_note[midi_note.pitch.midi] = n_i            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    overall_preds.append(new_note)            \n",
    "    \n",
    "    i1 = sample_with_temp(notes_prediction[0], notes_temp)\n",
    "    i2 = sample_with_temp(durations_prediction[0], duration_temp)    \n",
    "\n",
    "    note_result = int_to_note[i1]\n",
    "    duration_result = int_to_duration[i2]\n",
    "    \n",
    "    prediction_output.append([note_result, duration_result])\n",
    "\n",
    "    notes_input_sequence.append(i1)\n",
    "    durations_input_sequence.append(i2)\n",
    "    \n",
    "    if len(notes_input_sequence) > max_seq_len:\n",
    "        notes_input_sequence = notes_input_sequence[1:]\n",
    "        durations_input_sequence = durations_input_sequence[1:]\n",
    "        \n",
    "    if note_result == 'START':\n",
    "        break\n",
    "\n",
    "overall_preds = np.transpose(np.array(overall_preds)) \n",
    "print('Generated sequence of {} notes'.format(len(prediction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.set_yticks([int(j) for j in range(35,70)])\n",
    "plt.imshow(overall_preds[35:70,:], origin=\"lower\", cmap='coolwarm', vmin = -0.5, vmax = 0.5, extent=[0, max_extra_notes, 35,70])\n",
    "plt.xlabel(\"Note number\",fontsize=20)\n",
    "plt.ylabel(\"Pitch value (MIDI number)\",fontsize=20)\n",
    "plt.title(\"Probability distribution of the next possible note over time\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join('/output', 'output')\n",
    "\n",
    "midi_stream = stream.Stream()\n",
    "\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    note_pattern, duration_pattern = pattern\n",
    "    # pattern is a chord\n",
    "    if ('.' in note_pattern):\n",
    "        notes_in_chord = note_pattern.split('.')\n",
    "        chord_notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(current_note)\n",
    "            new_note.duration = duration.Duration(duration_pattern)\n",
    "            new_note.storedInstrument = instrument.Violoncello()\n",
    "            chord_notes.append(new_note)\n",
    "        new_chord = chord.Chord(chord_notes)\n",
    "        midi_stream.append(new_chord)\n",
    "    elif note_pattern == 'rest':\n",
    "    # pattern is a rest\n",
    "        new_note = note.Rest()\n",
    "        new_note.duration = duration.Duration(duration_pattern)\n",
    "        new_note.storedInstrument = instrument.Violoncello()\n",
    "        midi_stream.append(new_note)\n",
    "    elif note_pattern != 'START':\n",
    "    # pattern is a note\n",
    "        new_note = note.Note(note_pattern)\n",
    "        new_note.duration = duration.Duration(duration_pattern)\n",
    "        new_note.storedInstrument = instrument.Violoncello()\n",
    "        midi_stream.append(new_note)\n",
    "\n",
    "midi_stream = midi_stream.chordify()\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "new_file = 'output-' + timestr + '.mid'\n",
    "midi_stream.write('midi', fp=os.path.join(output_folder, new_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/kaggle/working/output/'+new_file\n",
    "fs = FluidSynth()\n",
    "fs.midi_to_audio(new_path, 'new_output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_attention:\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    im = ax.imshow(att_matrix[(seq_len-2):,], cmap='coolwarm', interpolation='nearest')    \n",
    "\n",
    "    # Minor ticks\n",
    "    ax.set_xticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n",
    "    ax.set_yticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n",
    "\n",
    "    # Gridlines based on minor ticks\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=1)    \n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(prediction_output) - seq_len))\n",
    "    ax.set_yticks(np.arange(len(prediction_output)- seq_len+2))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels([n[0] for n in prediction_output[(seq_len):]])\n",
    "    ax.set_yticklabels([n[0] for n in prediction_output[(seq_len - 2):]])\n",
    "    ax.xaxis.tick_top()    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"left\", va = \"center\", rotation_mode=\"anchor\")\n",
    "    plt.xlabel(\"sequence of generated notes\",fontsize=20)\n",
    "    plt.ylabel(\"The point of attention\",fontsize=20)\n",
    "    plt.title(\"The amount of attention given to the network hidden state\",fontsize=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"bach_846.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"bach_847.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"bach_850.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_score.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score = converter.parse(new_path).chordify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"new_output.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
